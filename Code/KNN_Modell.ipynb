{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/nicolas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nicolas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nicolas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (mit Stopwords entfernt) Accuracy: 0.6505793811969897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.76      0.67      0.62      8371\n",
      "weighted avg       0.77      0.65      0.62      8371\n",
      "\n",
      "KNN (ohne Stopwords entfernt) Accuracy: 0.6873730737068451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.49      0.62      4420\n",
      "           1       0.61      0.90      0.73      3951\n",
      "\n",
      "    accuracy                           0.69      8371\n",
      "   macro avg       0.73      0.70      0.68      8371\n",
      "weighted avg       0.74      0.69      0.68      8371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# knn_with_vs_without_stopwords.py\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess_with_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "def preprocess_without_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "\n",
    "df[\"processed_with\"] = df[\"tweet\"].apply(preprocess_with_stopwords)\n",
    "df[\"processed_without\"] = df[\"tweet\"].apply(preprocess_without_stopwords)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_with = vectorizer.fit_transform(df[\"processed_with\"])\n",
    "X_without = vectorizer.fit_transform(df[\"processed_without\"])\n",
    "y = df[\"Status\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN (mit Stopwords entfernt) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_without, y, test_size=0.2, random_state=42)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(X_train2, y_train2)\n",
    "y_pred2 = knn2.predict(X_test2)\n",
    "\n",
    "print(\"KNN (ohne Stopwords entfernt) Accuracy:\", accuracy_score(y_test2, y_pred2))\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Unigram Accuracy: 0.6505793811969897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.76      0.67      0.62      8371\n",
      "weighted avg       0.77      0.65      0.62      8371\n",
      "\n",
      "KNN Unigram+Bigram Accuracy: 0.5268187791183849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.13      0.22      4420\n",
      "           1       0.50      0.97      0.66      3951\n",
      "\n",
      "    accuracy                           0.53      8371\n",
      "   macro avg       0.67      0.55      0.44      8371\n",
      "weighted avg       0.68      0.53      0.43      8371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# knn unigram vs ngram\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "y = df[\"Status\"]\n",
    "\n",
    "vectorizer_uni = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_uni = vectorizer_uni.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "vectorizer_bi = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_bi = vectorizer_bi.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "X_train_uni, X_test_uni, y_train_uni, y_test_uni = train_test_split(X_uni, y, test_size=0.2, random_state=42)\n",
    "knn_uni = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_uni.fit(X_train_uni, y_train_uni)\n",
    "y_pred_uni = knn_uni.predict(X_test_uni)\n",
    "print(\"KNN Unigram Accuracy:\", accuracy_score(y_test_uni, y_pred_uni))\n",
    "print(classification_report(y_test_uni, y_pred_uni))\n",
    "\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X_bi, y, test_size=0.2, random_state=42)\n",
    "knn_bi = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_bi.fit(X_train_bi, y_train_bi)\n",
    "y_pred_bi = knn_bi.predict(X_test_bi)\n",
    "print(\"KNN Unigram+Bigram Accuracy:\", accuracy_score(y_test_bi, y_pred_bi))\n",
    "print(classification_report(y_test_bi, y_pred_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (L2 normalisiert) Accuracy: 0.6505793811969897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.76      0.67      0.62      8371\n",
      "weighted avg       0.77      0.65      0.62      8371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# knn l2 normalized \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "X_tfidf = tfidf.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "X_norm = normalizer.transform(X_tfidf)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN (L2 normalisiert) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 | Accuracy: 0.7335 | Weighted F1: 0.7204 | Macro F1: 0.7234 | Prediction Time: 1.9443 sec\n",
      "k = 2 | Accuracy: 0.7317 | Weighted F1: 0.7193 | Macro F1: 0.7222 | Prediction Time: 1.9509 sec\n",
      "k = 3 | Accuracy: 0.6615 | Weighted F1: 0.6285 | Macro F1: 0.6341 | Prediction Time: 2.1530 sec\n",
      "k = 5 | Accuracy: 0.6506 | Weighted F1: 0.6183 | Macro F1: 0.6239 | Prediction Time: 3.4183 sec\n",
      "k = 6 | Accuracy: 0.6552 | Weighted F1: 0.6292 | Macro F1: 0.6342 | Prediction Time: 3.4196 sec\n",
      "k = 8 | Accuracy: 0.6310 | Weighted F1: 0.5889 | Macro F1: 0.5956 | Prediction Time: 3.4491 sec\n",
      "k = 10 | Accuracy: 0.6085 | Weighted F1: 0.5534 | Macro F1: 0.5615 | Prediction Time: 3.4453 sec\n"
     ]
    }
   ],
   "source": [
    "# K tuning and TFIDF\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "k_values = [1, 2, 3, 5, 6, 8, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    elapsed = time.time() - start_time\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    results[k] = (acc, weighted_f1, macro_f1, elapsed)\n",
    "    print(f\"k = {k} | Accuracy: {acc:.4f} | Weighted F1: {weighted_f1:.4f} | Macro F1: {macro_f1:.4f} | Prediction Time: {elapsed:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7334846493847808\n",
      "Weighted F1: 0.720446774522609\n",
      "Macro F1: 0.7234050538212827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.51      0.67      4420\n",
      "           1       0.64      0.98      0.78      3951\n",
      "\n",
      "    accuracy                           0.73      8371\n",
      "   macro avg       0.80      0.75      0.72      8371\n",
      "weighted avg       0.81      0.73      0.72      8371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Weighted F1:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'n_neighbors': 5}\n",
      "Bestes durchschnittliches Accuracy (CV): 0.6510238952907768\n",
      "Klassifikationsbericht (CV auf dem gesamten Datensatz):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.53      0.69     21863\n",
      "           1       0.66      0.98      0.79     19988\n",
      "\n",
      "    accuracy                           0.75     41851\n",
      "   macro avg       0.81      0.76      0.74     41851\n",
      "weighted avg       0.82      0.75      0.73     41851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(knn, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_tfidf, y)\n",
    "\n",
    "print(\"Beste Parameter:\", grid.best_params_)\n",
    "print(\"Bestes durchschnittliches Accuracy (CV):\", grid.best_score_)\n",
    "\n",
    "y_pred_cv = grid.predict(X_tfidf)\n",
    "print(\"Klassifikationsbericht (CV auf dem gesamten Datensatz):\")\n",
    "print(classification_report(y, y_pred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'metric': 'cosine', 'n_neighbors': 13, 'weights': 'distance'}\n",
      "Bestes CV-Accuracy: 0.9221326164874553\n",
      "Test Accuracy: 0.9240234141679609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      4420\n",
      "           1       0.92      0.92      0.92      3951\n",
      "\n",
      "    accuracy                           0.92      8371\n",
      "   macro avg       0.92      0.92      0.92      8371\n",
      "weighted avg       0.92      0.92      0.92      8371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'cosine']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(knn, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Bestes CV-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
