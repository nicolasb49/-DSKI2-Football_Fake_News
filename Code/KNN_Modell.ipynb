{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:26:48.525332Z",
     "start_time": "2025-04-23T12:26:42.611483Z"
    }
   },
   "source": [
    "# all imports\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:29:43.172577Z",
     "start_time": "2025-04-23T12:26:54.300209Z"
    }
   },
   "source": [
    "# knn_with_vs_without_stopwords.py\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess_with_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "def preprocess_without_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "\n",
    "df[\"processed_with\"] = df[\"tweet\"].apply(preprocess_with_stopwords)\n",
    "df[\"processed_without\"] = df[\"tweet\"].apply(preprocess_without_stopwords)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_with = vectorizer.fit_transform(df[\"processed_with\"])\n",
    "X_without = vectorizer.fit_transform(df[\"processed_without\"])\n",
    "y = df[\"Status\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN (mit Stopwords entfernt) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_without, y, test_size=0.2, random_state=42)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(X_train2, y_train2)\n",
    "y_pred2 = knn2.predict(X_test2)\n",
    "\n",
    "print(\"KNN (ohne Stopwords entfernt) Accuracy:\", accuracy_score(y_test2, y_pred2))\n",
    "print(classification_report(y_test2, y_pred2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (mit Stopwords entfernt) Accuracy: 0.6475928801815792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.75      0.66      0.62      8371\n",
      "weighted avg       0.76      0.65      0.62      8371\n",
      "\n",
      "KNN (ohne Stopwords entfernt) Accuracy: 0.6830725122446542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.49      0.62      4420\n",
      "           1       0.61      0.89      0.73      3951\n",
      "\n",
      "    accuracy                           0.68      8371\n",
      "   macro avg       0.73      0.69      0.67      8371\n",
      "weighted avg       0.73      0.68      0.67      8371\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:31:13.716839Z",
     "start_time": "2025-04-23T12:29:43.287136Z"
    }
   },
   "source": [
    "# knn unigram vs ngram\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "y = df[\"Status\"]\n",
    "\n",
    "vectorizer_uni = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_uni = vectorizer_uni.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "vectorizer_bi = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_bi = vectorizer_bi.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "X_train_uni, X_test_uni, y_train_uni, y_test_uni = train_test_split(X_uni, y, test_size=0.2, random_state=42)\n",
    "knn_uni = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_uni.fit(X_train_uni, y_train_uni)\n",
    "y_pred_uni = knn_uni.predict(X_test_uni)\n",
    "print(\"KNN Unigram Accuracy:\", accuracy_score(y_test_uni, y_pred_uni))\n",
    "print(classification_report(y_test_uni, y_pred_uni))\n",
    "\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X_bi, y, test_size=0.2, random_state=42)\n",
    "knn_bi = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_bi.fit(X_train_bi, y_train_bi)\n",
    "y_pred_bi = knn_bi.predict(X_test_bi)\n",
    "print(\"KNN Unigram+Bigram Accuracy:\", accuracy_score(y_test_bi, y_pred_bi))\n",
    "print(classification_report(y_test_bi, y_pred_bi))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Unigram Accuracy: 0.6475928801815792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.75      0.66      0.62      8371\n",
      "weighted avg       0.76      0.65      0.62      8371\n",
      "\n",
      "KNN Unigram+Bigram Accuracy: 0.5258630987934536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.13      0.22      4420\n",
      "           1       0.50      0.97      0.66      3951\n",
      "\n",
      "    accuracy                           0.53      8371\n",
      "   macro avg       0.67      0.55      0.44      8371\n",
      "weighted avg       0.68      0.53      0.43      8371\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:32:35.923989Z",
     "start_time": "2025-04-23T12:31:13.772083Z"
    }
   },
   "source": [
    "# knn l2 normalized \n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "X_tfidf = tfidf.fit_transform(df[\"processed_tweet\"])\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "X_norm = normalizer.transform(X_tfidf)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN (L2 normalisiert) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (L2 normalisiert) Accuracy: 0.6475928801815792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52      4420\n",
      "           1       0.58      0.97      0.72      3951\n",
      "\n",
      "    accuracy                           0.65      8371\n",
      "   macro avg       0.75      0.66      0.62      8371\n",
      "weighted avg       0.76      0.65      0.62      8371\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:34:38.440463Z",
     "start_time": "2025-04-23T12:32:36.011323Z"
    }
   },
   "source": [
    "# K tuning and TFIDF\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "k_values = [1, 2, 3, 5, 6, 8, 10]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    start_time = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    elapsed = time.time() - start_time\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    results[k] = (acc, weighted_f1, macro_f1, elapsed)\n",
    "    print(f\"k = {k} | Accuracy: {acc:.4f} | Weighted F1: {weighted_f1:.4f} | Macro F1: {macro_f1:.4f} | Prediction Time: {elapsed:.4f} sec\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 | Accuracy: 0.7214 | Weighted F1: 0.7182 | Macro F1: 0.7195 | Prediction Time: 6.7412 sec\n",
      "k = 2 | Accuracy: 0.6950 | Weighted F1: 0.6952 | Macro F1: 0.6949 | Prediction Time: 6.5911 sec\n",
      "k = 3 | Accuracy: 0.6776 | Weighted F1: 0.6602 | Macro F1: 0.6640 | Prediction Time: 7.1586 sec\n",
      "k = 5 | Accuracy: 0.6476 | Weighted F1: 0.6154 | Macro F1: 0.6211 | Prediction Time: 6.9955 sec\n",
      "k = 6 | Accuracy: 0.6527 | Weighted F1: 0.6267 | Macro F1: 0.6316 | Prediction Time: 6.8412 sec\n",
      "k = 8 | Accuracy: 0.6276 | Weighted F1: 0.5857 | Macro F1: 0.5924 | Prediction Time: 6.8583 sec\n",
      "k = 10 | Accuracy: 0.6064 | Weighted F1: 0.5504 | Macro F1: 0.5585 | Prediction Time: 6.8790 sec\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:36:00.233118Z",
     "start_time": "2025-04-23T12:34:38.493482Z"
    }
   },
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Weighted F1:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721419185282523\n",
      "Weighted F1: 0.7182106858790367\n",
      "Macro F1: 0.7195079153792394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.60      0.70      4420\n",
      "           1       0.66      0.85      0.74      3951\n",
      "\n",
      "    accuracy                           0.72      8371\n",
      "   macro avg       0.74      0.73      0.72      8371\n",
      "weighted avg       0.74      0.72      0.72      8371\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:39:58.690751Z",
     "start_time": "2025-04-23T12:36:00.278979Z"
    }
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(knn, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_tfidf, y)\n",
    "\n",
    "print(\"Beste Parameter:\", grid.best_params_)\n",
    "print(\"Bestes durchschnittliches Accuracy (CV):\", grid.best_score_)\n",
    "\n",
    "y_pred_cv = grid.predict(X_tfidf)\n",
    "print(\"Klassifikationsbericht (CV auf dem gesamten Datensatz):\")\n",
    "print(classification_report(y, y_pred_cv))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'n_neighbors': 1}\n",
      "Bestes durchschnittliches Accuracy (CV): 0.7218226405179056\n",
      "Klassifikationsbericht (CV auf dem gesamten Datensatz):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     21863\n",
      "           1       1.00      1.00      1.00     19988\n",
      "\n",
      "    accuracy                           1.00     41851\n",
      "   macro avg       1.00      1.00      1.00     41851\n",
      "weighted avg       1.00      1.00      1.00     41851\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-23T12:42:00.931026Z"
    }
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(preprocess)\n",
    "\n",
    "X = df[\"processed_tweet\"]\n",
    "y = df[\"Status\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'cosine']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(knn, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Bestes CV-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:40:05.539222300Z",
     "start_time": "2025-04-20T16:36:44.306170Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
    "import cupy as cp\n",
    "import cudf\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Hilfsfunktionen\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "additional_stopwords = {\"al\", \"god\", \"https\", \"ahly\", \"http\"}\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "def preprocess_with_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
    "        if lemma not in stop_words:\n",
    "            processed_words.append(lemma)\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "def preprocess_without_stopwords(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
    "\n",
    "# ðŸ“„ CSV laden\n",
    "df = pd.read_csv(\"./assets/combined.csv\")\n",
    "df.dropna(subset=[\"tweet\", \"Status\"], inplace=True)\n",
    "\n",
    "df[\"processed_with\"] = df[\"tweet\"].apply(preprocess_with_stopwords)\n",
    "df[\"processed_without\"] = df[\"tweet\"].apply(preprocess_without_stopwords)\n",
    "\n",
    "# TF-IDF (CPU)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_with = vectorizer.fit_transform(df[\"processed_with\"])\n",
    "X_without = vectorizer.fit_transform(df[\"processed_without\"])\n",
    "y = df[\"Status\"]\n",
    "\n",
    "# ðŸ” In GPU-Formate umwandeln\n",
    "X_with_gpu = cp.sparse.csr_matrix(X_with)\n",
    "X_without_gpu = cp.sparse.csr_matrix(X_without)\n",
    "y_gpu = cudf.Series(y.values)\n",
    "\n",
    "# ðŸš€ GPU-KNN mit Stopwords\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_gpu, y_gpu, test_size=0.2, random_state=42)\n",
    "knn_gpu = cuKNN(n_neighbors=5)\n",
    "knn_gpu.fit(X_train, y_train)\n",
    "y_pred = knn_gpu.predict(X_test)\n",
    "\n",
    "print(\"GPU-KNN (mit Stopwords entfernt) Accuracy:\", accuracy_score(y_test.to_numpy(), y_pred.to_numpy()))\n",
    "print(classification_report(y_test.to_numpy(), y_pred.to_numpy()))\n",
    "\n",
    "# ðŸš€ GPU-KNN ohne Stopwords\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_without_gpu, y_gpu, test_size=0.2, random_state=42)\n",
    "knn_gpu2 = cuKNN(n_neighbors=5)\n",
    "knn_gpu2.fit(X_train2, y_train2)\n",
    "y_pred2 = knn_gpu2.predict(X_test2)\n",
    "\n",
    "print(\"GPU-KNN (ohne Stopwords entfernt) Accuracy:\", accuracy_score(y_test2.to_numpy(), y_pred2.to_numpy()))\n",
    "print(classification_report(y_test2.to_numpy(), y_pred2.to_numpy()))\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score, classification_report\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcuml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneighbors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KNeighborsClassifier \u001B[38;5;28;01mas\u001B[39;00m cuKNN\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcupy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcp\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcudf\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'cuml'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
